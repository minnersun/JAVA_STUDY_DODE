## 索引（luncene）

--------

### 索引文件

##### 索引定义

> 根据一定规则排序，具备一定数据结构的，能够==快速定位==数据的一批数据

##### 索引结构的应用（mysql（B+TREE））

> 在树中有不同的节点，每一个节点都是技术底层的文件系统管理的文件单元Unit
>
> 数据单元可以是4K，也可以是8K，量越大，数据越多，磁头寻址越少
>
> 树状，第一个节点（根节点），中间节点（枝节点），最后一个节点（叶节点）



### 全文检索，索引文件

##### 倒排索引算法

> 分词/词项/词条化

> `query`：查询
>
> `document`：文档
>
> `field`：域属性
>
> `Tream`：词项

> 倒排索引算法步骤
>
> > 1.收集数据源封装对象
> >
> > 2.对所有的数据对象`document`中的内容进行属性的分词计算
> >
> > 3.计算结果是重复的分词则进行合并
> >
> > 4.将数据`document`合并输出存储，在磁盘形成索引文件结构

> 倒排索引详解
>
> > 略



### luncene

#### 概括

> `luncene`是全文检索引擎包,它的出现极大的提升了搜索的开发效率
>
> 特点
>
> > 基于java，对其他语言不友好
> >
> > 索引稳定
> >
> > 提供丰富的搜索功能
> >
> > > 词项，多域（所属性）搜索，布尔搜索，范围搜索

> luncene底层使用的是浅查询
>
> > 浅查询
> >
> > > 多级查询,第一级不获取数据,获取的是数据的标识(id数据,集合)
> > >
> > > 下一级查询才获取数据
> >
> > 深查询
> >
> > > 第一级查询就将需要的数据直接获取到最后的页数和条数对应的内容
> > >
> > > 前面所有的数据虽然读取了,但是并不适用

#### luncene的分词实现

###### lucene实现的分词器:

> `WhitespaceAnalyzer`:空格分词器 
>
> `StandardAnalyzer`:标准分词器(一个字符分一个词项)
>
> `SimpleAnalyzer`:一个标点符号分一个词项(句)
>
> `SmartChineseAnalyzer`:智能中文分词器
>
> `IKAnalzyer`:常用的一个中文分词器,支持词语的扩展和停用



> 索引文件的创建，核心分词计算，分词计算的情况比较多样化

###### luncene测试有关的依赖

````xml
		<dependency> <!-- 查询相关jar包 -->
			<groupId>org.apache.lucene</groupId>
			<artifactId>lucene-queryparser</artifactId>
			<version>6.0.0</version>
		</dependency>
		<dependency> <!-- lucene自带只能中文分词器jar包 -->
			<groupId>org.apache.lucene</groupId>
			<artifactId>lucene-analyzers-smartcn</artifactId>
			<version>6.0.0</version>
		</dependency>
		<dependency> <!-- 测试用到的lucene工具包 -->
			<groupId>org.apache.lucene</groupId>
			<artifactId>lucene-analyzers-common</artifactId>
			<version>6.0.0</version>
		</dependency>
		<dependency> <!-- 测试用到的lucene核心包 -->
			<groupId>org.apache.lucene</groupId>
			<artifactId>lucene-core</artifactId>
			<version>6.0.0</version>
		</dependency>
````

###### 分词器测试代码

````java
		package cn.tedu.test;
		
		public class AnalyzerTest {
			//获取分词器计算的此项字符串的方法,term:词项
			//Analyzer 不同实现类分词器逻辑不同
			//String 分词计算的文本源数据
			public void printTerm(Analyzer analyzer,String msg){
				//获取流对象
				StringReader reader=new StringReader(msg);
				//利用分词器a对reader进行分词处理,处理结果就是一个具备
				//分词所有内容的流对象,从这个对象中获取字符串属性
				//打印
				TokenStream token = 
					analyzer.tokenStream("test", reader);
				//重置流对象
				try{
					token.reset();
					//获取每一个分词的字符串属性
					CharTermAttribute attribute = token.getAttribute(CharTermAttribute.class);
					while(token.incrementToken()){
						//打印attribute
						System.out.println(attribute.toString());
					}
				}catch(Exception e){
					e.printStackTrace();
				}
				//fieldName 分词中携带的属性 域(document的属性名称)
			}
			//测试同一个字符串使用不同分词器的打印分词效果
			@Test
			public void run(){
				//生成分词器
				Analyzer a1=new StandardAnalyzer();
				Analyzer a2=new SimpleAnalyzer();
				Analyzer a3=new WhitespaceAnalyzer();
				Analyzer a4=new SmartChineseAnalyzer();
				//生成字符串源数据
				String msg="魔兽世界需要排队5小时,终于登上了";
				System.out.println("*********标准**********");
				printTerm(a1, msg);
				System.out.println("*********简单**********");
				printTerm(a2, msg);
				System.out.println("*********空格**********");
				printTerm(a3, msg);
				System.out.println("*********智能中文**********");
				printTerm(a4, msg);
		}
     }

````



#### IK分词器

###### 依赖

```xml
    <dependency>
        <groupId>ik</groupId>
        <artifactId>ik</artifactId>
        <version>ik</version>
        <scope>system</scope>
        <systemPath>C://IKAnalyzer2012_u6.jar</systemPath>
    </dependency>

```

###### IKAnalyzer.cfg.xml

> 将此文件放于`src/main/resources`目录下
>
> 如果有多个文件,以分号隔开
>
> 词典文件要和配置文件在同一个目录下
>
> 字典文件和配置文件的编解码要一致

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">  
<properties>  
	<comment>IK Analyzer 扩展配置</comment>
	<!--用户可以在这里配置自己的扩展字典 
	<entry key="ext_dict">ext.dic;</entry> 
	-->
	<!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords">stopword.dic;</entry> 
</properties>
```

###### 创建索引代码

> 思考三个问题
>
> > - TextField和StringField区别
> > - Store YES/NO的含义
> > - **POINT的使用

```java
	public class CreateIndex {
		@Test
		public void createIndex() throws Exception{
			//准备索引文件保存数据的文件夹
			Path path = Paths.get("c://index01");
			//将路径传递给lucene对象使用
			Directory dir=FSDirectory.open(path);
			//封装数据document
			Document doc1=new Document();
			Document doc2=new Document();
			//productName,productImage,
			//productPrice,productCategory
			doc1.add(new TextField("productName", 
					"三星固态硬盘", Store.YES));
			//name 属性名,value 属性值,store是否存储索引
			doc1.add(new StringField("productImage",
					"www.img.com",Store.YES));
			doc1.add(new DoublePoint("productPrice", 5000));
			doc1.add(new TextField("productCat","电脑硬件"
					,Store.YES));
			doc2.add(new TextField("productName", 
					"金士顿固态硬盘", Store.YES));
			//name 属性名,value 属性值,store是否存储索引
			doc2.add(new StringField("productImage",
					"www.easymall.com",Store.YES));
            
			doc2.add(new DoublePoint("productPrice", 355));
			doc2.add(new StringField("productPrice","355元",
					Store.YES));
			doc2.add(new TextField("productCat","电脑硬件"
					,Store.NO));
			//TextField和StringField什么区别
			//Store.YES/NO什么作用
			//数字特性的field和字符串有什么关系
			//创建输出流对象writer
				//config可以定义writer一些输出格式
			IndexWriterConfig config=new 
					IndexWriterConfig(new IKAnalyzer6x());
			//将索引文件输出到c://index01,运行第二次的覆盖方式可以
			//由config定义
			//create:每次调用都覆盖原有内容
			//append:每次调用都将新数据追加到原有内容索引
			//create_or_append:无则建,有则追加
			config.setOpenMode(OpenMode.CREATE);
			//dir config
			IndexWriter writer=new IndexWriter(dir, config);
			//利用writer将数据输出到磁盘形成索引文件
			writer.addDocument(doc1);
			writer.addDocument(doc2);
			writer.commit();
	}
}

```

##### 问题解决

> TextField和StringField区别
>
> > TextField需要对域的值进行分词的计算
> >
> > StringField不需要对域的值进行分词计算(productId,url)
>
> Store YES/NO
>
> > YES:表示索引文件中在处理document时需要存储数据
> >
> > NO:表示document域数据不存储在索引中
> >
> > 影响:查询的结果,对于NO的数据是获取不到的
>
> **POINT的使用
>
> > Point类型只保留一个field的数字特性属性(范围查询)
> >
> > 即不计算分词,也不存储 如果需要使用这个field的值
> >
> > 可以配合同名域的StringField类型 对一个域做多个属性的添加

#### lucene的搜索功能

###### 词项查询

> 提供一个封装号的Term对象(域名,词项值),利用底层分词结果比对索引中的分词合并结果,要不找到对应值,要不找不到.所有其他查询功能的基础

```java
	/*
	 * 词项查询,利用传递的参数封装词项
	 * 对比分词结果获取指定的document 集合
	 */
	@Test
	public void termQuery() throws Exception{
		//指定文件夹索引位置
		Directory dir=FSDirectory
				.open(Paths.get("c://index01"));
		//根据文件夹位置生成reader流 创建搜索对象
		IndexReader reader=DirectoryReader.open(dir);
		IndexSearcher search=new IndexSearcher(reader);
		//准备搜索条件
		Term term=new Term("productName","硬盘");
		Query query=new TermQuery(term);
		//搜索封装了 document大量标识数据的对象(没有源数据)
		//document 评分等,根据查询条件的不同,自动计算评分
		//词项中,根据字符串匹配长度,长度越大,评分越高
		//查询的条数一共多少条等
		TopDocs topDoc = search.search(query, 10);
		//query查询条件,result 表示查询前几条 10
		System.out.println("最高分:"+topDoc.getMaxScore());
		System.out.println("一共获取数据:"+topDoc.totalHits);
		//利用浅查询得到的评分对象拿到documentId
		ScoreDoc[] scoreDocs = topDoc.scoreDocs;
		for (ScoreDoc scoreDoc : scoreDocs) {
			//每次循环都获取返回结果中一个document评分相关内容
			System.out.println("当前docid:"+scoreDoc.doc);
			System.out.println("当前doc评分:"+scoreDoc.score);
			//利用documentId获取源数据 拿不到Store.NO的数据
			Document doc=search.doc(scoreDoc.doc);
			//解析所有属性值
			System.out.println("productName"+doc.get("productName"));
			System.out.println("productImage"+doc.get("productImage"));
	}}

```



###### 多域查询

> 可以在一个查询条件中,真多多个不同的域名实现搜索的功能.
>
> 基于termQuery实现的,可以对一个关键字进行解析形成多个词项,对多个与做排列组合的结果,获取最终并集

```java
	@Test
	public void multilFieldQuery() throws Exception{
		Directory dir=FSDirectory
				.open(Paths.get("c://index01"));
		IndexReader reader=DirectoryReader.open(dir);
		IndexSearcher search=new IndexSearcher(reader);
		//生成多域查询条件
			//生成解析器 解析字符串形成多个分词的结果
		String[] fields={"productName","productCat"};
		Analyzer analyzer=new IKAnalyzer6x();
		MultiFieldQueryParser parser=new 
				MultiFieldQueryParser(fields, analyzer);
		//利用解析器生成多域查询条件
		Query query=parser.parse("三星固态硬盘是否会爆炸");
		/*	productName productCat
		 *  三星 term(productName ,三星) term(productCat,三星)
			...
			以上任何一个词项查询的结果集 做并集处理
		 */
				
		TopDocs topDoc = search.search(query, 10);
		System.out.println("最高分:"+topDoc.getMaxScore());
		System.out.println("一共获取数据:"+topDoc.totalHits);
		ScoreDoc[] scoreDocs = topDoc.scoreDocs;
		for (ScoreDoc scoreDoc : scoreDocs) {
			//每次循环都获取返回结果中一个document评分相关内容
			System.out.println("当前docid:"+scoreDoc.doc);
			System.out.println("当前doc评分:"+scoreDoc.score);
			//利用documentId获取源数据 拿不到Store.NO的数据
			Document doc=search.doc(scoreDoc.doc);
			//解析所有属性值
			System.out.println("productName"+doc.get("productName"));
			System.out.println("productImage"+doc.get("productImage"));
		}
	}
```



###### 布尔查询

> 当系统存在多个查询条件时query,单独运行都能对应到一批结果集.可以利用布尔查询,封装查询条件获取所有query之间的逻辑集合

> `MUST`必须包含这个子条件结果
>
> `MUST_NOT`必须不包含子条件结果\
>
> `SHOULD`可以包含子条件结果
>
> `FILTER和MUST` 对应这个子条件的所有结果没有评分

```java
	@Test
	public void booleanQuery() throws Exception{
		Directory dir=FSDirectory
				.open(Paths.get("c://index01"));
		IndexReader reader=DirectoryReader.open(dir);
		IndexSearcher search=new IndexSearcher(reader);
		//准备boolean条件的子条件termQuery
		Query query1=new TermQuery(new Term("productName","硬盘"));
		Query query2=new TermQuery(new Term("productName","三星"));
		//利用query1 2 封装子条件
		BooleanClause bc1=new BooleanClause(query1,Occur.FILTER);
		BooleanClause bc2=new BooleanClause(query2,Occur.MUST_NOT);
		//occur决定了查询结果与当前条件的逻辑关系
		/*MUST:查询结果必须包含这个条件的结果
		 *MUST_NOT:查询结果必须不包含这个条件的结果
		 *SHOULD:可包含可不包含,当他与MUST同时存在时,不生效
		 *FILTER:MUST效果一样的,但是通过FILTER子条件查询的结果没有评分
		 */
		Query query=new BooleanQuery.
				Builder().add(bc1).add(bc2).build();
				
		TopDocs topDoc = search.search(query, 10);
		System.out.println("最高分:"+topDoc.getMaxScore());
		System.out.println("一共获取数据:"+topDoc.totalHits);
		ScoreDoc[] scoreDocs = topDoc.scoreDocs;
		for (ScoreDoc scoreDoc : scoreDocs) {
			//每次循环都获取返回结果中一个document评分相关内容
			System.out.println("当前docid:"+scoreDoc.doc);
			System.out.println("当前doc评分:"+scoreDoc.score);
			//利用documentId获取源数据 拿不到Store.NO的数据
			Document doc=search.doc(scoreDoc.doc);
			//解析所有属性值
			System.out.println("productName"+doc.get("productName"));
			System.out.println("productImage"+doc.get("productImage"));
		}
	}
```



###### 范围查询

> 在数据中存在非常多的与数字特性有关的内容,价钱,容量,尺寸等
>
> 域属性中如果需要范围查询对应搜索到document,必须制定**Point类型
>
> **Point不做分词,也不做索引的存储
>
> 配合StringField来获取想要的数据内容

```java
	@Test
	public void rangeQuery() throws Exception{
		Directory dir=FSDirectory
				.open(Paths.get("c://index01"));
		IndexReader reader=DirectoryReader.open(dir);
		IndexSearcher search=new IndexSearcher(reader);
		//生成对price价钱做范围查询的query
		Query query=DoublePoint.newRangeQuery
				("productPrice", 555, 8000);
		TopDocs topDoc = search.search(query, 10);
		System.out.println("最高分:"+topDoc.getMaxScore());
		System.out.println("一共获取数据:"+topDoc.totalHits);
		ScoreDoc[] scoreDocs = topDoc.scoreDocs;
		for (ScoreDoc scoreDoc : scoreDocs) {
			//每次循环都获取返回结果中一个document评分相关内容
			System.out.println("当前docid:"+scoreDoc.doc);
			System.out.println("当前doc评分:"+scoreDoc.score);
			//利用documentId获取源数据 拿不到Store.NO的数据
			Document doc=search.doc(scoreDoc.doc);
			//解析所有属性值
			System.out.println("productName"+doc.get("productName"));
			System.out.println("productImage"+doc.get("productImage"));
			System.out.println("price"+doc.get("productPrice"));
		}
	}
```



######  模糊查询

> 例如:数据存储的是trump 特朗普,查询时tramp也可以将数据查询出来,使用模糊查询 精确度匹配不高,日和曰
>
> `Query query=new FuzzyQuery(new Term("productName","samsong"))`



###### 通配符查询

> 可以使用?代替一个字符
>
> `Query query=new WildcartQuery(new Term("productName","三?"));`
>
> 可以查到所有词项中以三开头的2个字符的数据



### luncene问题

> 对其他语言不方便，只支持java
>
> 不能实现对索引的保护和管理



### Rest风格（RestFull）

> `elasticsearch`完全基于http协议的rest风格设计的技术,
>
> `springmvc`也是支持REST风格的

> Rest风格举例
>
> > 不符合风格演示url(对user做操作)
> >
> > > `http://localhost:10000/user/manage/update`
> > >
> > > `http://localhost:10000/user/manage/save`
> >
> > 符合Rest风格的url
> >
> > > `http://localhost:10000/user/manage/{userId}`
>
> 
>
> `elasticsearch`遵循REST风格
>
> > `put:http://elasticsearch/index01 新增索引`
> >
> > `get:http://elasticsearch/index01 查询索引`
> >
> > `delete:http://elasticsearch/index01 删除索引`

