## spark笔记整理

----

### Spark集群搭建

> 详见课前资料

> Spark的使用模式
>
> > **Local**	-本地单机模式
> >
> > > 用于测试
> >
> > **Standalone Spark**	-集群模式
> >
> > > 在这个模式下，Spark集群的**资源管理由Spark的Master来负责**
> >
> > **On Yarn**
> >
> > > Spark集群的**资源管理交给Yarn的ReourceManager来负责**
> > >
> > > 使用On Yarn模式，**可以达到针对集群总的资源管理，可以提高集群利用率**





### RDD操作

> 课前资料中有案例

#### Transformation懒操作

> `union()`
>
> > 取**并集**
>
> `intersection()`
>
> > 取**交集**
>
> `subtract()`
>
> > 取**差集**
>
> `distinct`
>
> > **去重**
>
> `groupByKey`
>
> > 操作的数必须是一个**二元tuple**
> >
> > 根据**键排序**
>
> `reduceByKey()`
>
> > 操作的数必须是一个**二元tuple**
> >
> > **相同的key，value间进行计算**
>
> `aggregateByKey(zeroValue)(func1,func2)`
>
> > **可以作为`reduceByKey`的优化**
> >
> > `zeroValue`
> >
> > > 表示初始值，初始值会参与func1的计算
> >
> > 分区内
> >
> > >  按key分组，把每组的值进行fun1的计算
> >
> > **每个分区每组的计算结果按fun2进行计算**
>
> `sortByKey()`
>
> > 根据**key**值排序
>
> `cartesian()`
>
> > 求**笛卡儿积**
>
> `join`
>
> > **连接**操作
> >
> > 将输入数据集**(K,V)**和另外一个数据集**(K,W)**进行Join， 得到**(K, (V,W))**
>
> `coalesce`
>
> > **扩大或缩小分区**

#### Actions操作

> `reduce`
>
> > **并行整合所有RDD数据**
> >
> > 比如**求和**
>
> `collect`
>
> > 返回RDD所有元素
> >
> > > 将所有分区的数据获取到一起，组成一个数组返回
> >
> > **容易造成内存的溢出，生产环境中慎用**
>
> `count`
>
> > 统计RDD中元素的**个数**
>
> `first`
>
> > 返回第一个元素，与take(1)相似
>
> `take()`
>
> > **获取前几个数据**
>
> `takeOrdered()`
>
> > 先将RDD中的元素进行**升序排序**
> >
> > **然后取前n个**
>
> `top()`
>
> > 先将RDD中的元素进行**降序排序**
> >
> > **然后取前n个**
>
> `saveAsTextFile`
>
> > 按照**文本方式**保存分区数据
>
> `countByKey`
>
> > 与count相似，**以key为单位进行计数**
>
> `foreach()`
>
> > **遍历**RDD中的元素





### Spark的DAG和RDD间的依赖关系

#### DAG	-有向无环图

> Spark会根据用户提交的**计算逻辑**中的**RDD的转换和动作来生成RDD之间的依赖关系**
>
> > 同时这个计算链也就生成了**逻辑上的DAG**
>
> DAG中记录了**RDD之间的依赖关系**

#### RDD之间的依赖关系

##### RDD的依赖关系分为两种

###### Shuffle

> Spark也是**有Shuffle**过程的，已经尽量**避免产生Shuffle**

###### 窄依赖

> **一对一关系**
>
> > 每一个**parent RDD**最多被一个 **child RDD**使用
>
> > 针对窄依赖，通常**没有Shuffle操作**
> >
> > > 通常这些方法没有**分区操作**

###### 宽依赖

> **一对多关系**
>
> > 多个**child RDD**会依赖同一个**parent RDD**的Partition
>
> > 底层会产生**Shuffle过程**





### DAG的生成与Stage的划分

> **原始的RDD**通过一系列的转化就形成了**DAG**
>
> > 同时也实现了RDD的**容错性**
>
> Spark的Stage阶段
>
> >Spark在执行任务（job）时，首先会根据**依赖关系**，将DAG划分为**不同的阶段（Stage）**
>
> > stage**内部**可以执行**流水线优化**
> >
> > 而在**stage之间没办法执行流水线优化**，因为有**shuffle**
> >
> > > 但是这种机制已经尽力的去**避免了shuffle**

#### 处理流程

> Spark在执行**Transformation**类型操作时都**不会立即执行**，而是**懒执行（计算）**
>
> 执行若干步的Transformation类型的操作后，**一旦遇到Action类型操作时，才会真正触发执行（计算）**
>
> 执行时，**从当前Action方法向前回溯**，如果遇到的是**窄依赖则应用流水线优化**，继续向前找，直到**碰到某一个宽依赖**
>
> > 因为**宽依赖必须要进行shuffle**，无法实现优化，所以将这一次段执行过程组装为一个**stage**
>
> **再**从当前宽依赖开始**继续向前找**
>
> > **重复刚才的步骤，从而将整个DAG还分为若干的stage**



#### Spark架构

> **SparkContext**
>
> > 程序员编写的Spark的启动程序
> >
> > 每一个**Driver**中都会有一个**SparkContext**对象
>
> > **SparkContext**和**Cluster Manger**交互
> >
> > > 目的是为**Driver**的**Task**启动和执行**申请资源**
> >
> > **CM收到请求后**，在符合条件的WorkNode上启动，并通知**SC**相关信息
>
> **Cluster Manager**
>
> > Spark集群的**管理器**
> >
> > **负责整个集群的资源管理**
>
> > 监控Task的运行状态，以及Task的失败恢复
> >
> > 心跳机制，可以监控每一台节点的状态
>
> **WorkerNode**
>
> > Spark的工作服务器
> >
> > 用于**启动和管理Executor**进程，在进程中处理**Task任务(分区数据)**
>
> > Task和Task之间的数据通信，比如Shuffle过程

##### Spark管理模式与YARND对比

> 看笔记





### VSM	-向量空间模型算法

> VSM算法的基础
>
> > **相似度**的概念
>
> 词汇检索
>
> > 根据查询条件和文档之间的相似度来完成排名

#### 平均值

> > 均值用于描述总体的平均情况

#### 方差

> > 方差用于描述数据集的**散布程度**
> >
> > > **方差越大数据集越散布（越不稳定）**
> > >
> > > **方差越小，数据集越稳定**
>
> > 数学公式
> >
> > > $\frac{1}{n}$$\sum_{i=0}^n$（x-$\overline{x}$）^2

#### 协方差

> > 协方差用于描述变量之间的**相关性**
> >
> > > 协方差> 0
> > >
> > > > 表示变量之间正相关
> > >
> > > 协方差=0
> > >
> > > > 表示变量之间没有任何关系
> > >
> > > 协方差<0
> > >
> > > > 表示变量之间是负相关
> >
> > > **不能使用协方差的大小，来描述相关性的强弱**
>
> > 数学公式
> >
> > > $\frac{1}{n-1}$$\sum_{i=0}^n$（x-$\overline{x}$）（y-$\overline{y}$）

#### 相关系数

> > 可以描述变量之间的**相关性强弱**
> >
> > > -1<相关系数<1
> >
> > > 相关系数>0
> > >
> > > > 变量之间正相关
> > > >
> > > > **相关系数越靠近1，表明正相关性越强**
> > >
> > > 相关系数=0
> > >
> > > > **变量之间不相关**
> > >
> > > 相关系数<0
> > >
> > > > 变量之间负相关
> > > >
> > > > **相关系数越靠近1，表明负相关性越强**
>
> > 公式
> >
> > > r = $\frac{8xy}{\sqrt{8xx}$\sqrt{8yy}}$

#### 向量之间的夹角余弦

> 公式
>
> > cos($\theta$)=$\frac{a*b}{|a||b|}$=cos($\theta$)=$\frac{\sum_{i=1}^na_i*b_i}{\sqrt{\sum_{1}^n(a_i)^2}*\sqrt{\sum_{1}^n(b_i)^2}}$
> >
> > > 例如
> > >
> > > > a(1,2)
> > > >
> > > > b(3,4)
> > > >
> > > > > r = $\frac{1*3+2*4}{\sqrt{1+4}\sqrt{9+16}}$
>
> > cos($\theta$)=1
> >
> > > 完全正相关
> >
> > 0<cos($\theta$)<1
> >
> > > 一定程度的正相关
> > >
> > > **夹角越大，相关性在减弱**
> >
> > cos($\theta$)=0，$\theta$=90
> >
> > > 不相关
> >
> > -1<cos($\theta$)<0
> >
> > > 一定程度负相关
> >
> > cos($\theta$)=-1，$\theta$=180
> >
> > > 完全负相关

