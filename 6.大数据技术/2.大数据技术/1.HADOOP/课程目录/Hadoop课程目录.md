## 课程目录

### 第一天

#### 大数据

###### 大数据特点

> 6V
>
> > 数据体量大
> >
> > 数据种类和样式多
> >
> > 数据量的增长速度越来越来
> >
> > 数据价值密度低
> >
> > 数据的真实性
> >
> > 数据的连通性
>
> 补充
>
> > 数据的动态性 
> >
> > 可视化
> >
> > 合法性（隐私性）

###### 大数据相关技术

> 数据收集
>
> 数据存储
>
> ###### 数据清洗
>
> ###### 数据分析
>
> 数据挖掘

###### 大数据，云计算，物联网，人工智能之间的关系

> 略



#### Hadoop

###### Hadoop版本

> `Hadoop1.X`
>
> > 只包含`Common`，`HDFS`，`MapReduce`
>
> `Hadoop2.X`
>
> > 包含Common，`HDFS`，`Mapreduce`，`YARN`
>
> `Hadoop3.X`
>
> > 包含Common，`HDFS`，`MapReduce`，`YARN`，`Ozone`

###### Hadoop模块

> `Common`
>
> > 基本模块
>
> `HDFS`
>
> > 用于分布式存储
>
> `YARN`
>
> > 用于完成任务调度和资源管理
>
> `MapReduce`
>
> > 基于YARN进行分布式计算
>
> `Ozone`
>
> > 对象存储
>
> `Submarine`
>
> > Hadoop的机器学习引擎	- 2019.3



#### HDFS

> Hadoop用于进行分布式存储的组件

##### 概述

> `HDFS`需要有一个节点来负责**管理**
>
> 多个节点负责**存储** 
>
> HDFS是一个典型的**主从结构**

> 负责管理的节点称之为**NameNode**，负责存储的节点称之为**DataNode**
>
> 在HDFS中，存储数据的时候，会将**数据进行切分**，切出多个数据存放到多个节点上
>
> HDFS会将切出的数据进行**备份**，每一个备份称之为是一个**副本(replication)**，在HDFS中，默认**副本数量为3**

##### Block

> 在HDFS中，数据是以Block的形式来进行存储的
>
> > 在磁盘中存储的大小，实际上是文件的大小，所以**block的大小实际上与块的大小一致**
>
> 在HDFS中，会自动对Block进行**编号**
>
> 切块的**意义**
>
> > 为了能够存储超大文件
> >
> > 为了能够快速的进行备份

##### NameNode

> NameNode是HDFS中的主节点，负责记录**元数据（metadata）**和管理**DataNode**
>
> **元数据(metadata)**主要包含
>
> > 文件的大小，文件的存储路径
> >
> > Block的大小，BlockID
> >
> > Block和DataNode的映射关系
> >
> > 副本的数量
> >
> > 文件的权限

> NameNdoe会将**元数据**存储在**内存**以及**磁盘**上
>
> > 存储在内存上是为了查询快
> >
> > 存储在磁盘上是为了数据的恢复
> >
> > > 元数据在磁盘上的存储位置由**hadoop.tmp.dir**来决定
>
> NameNode在收到写操作的时候，会先记录到**edits_inprogress**文件中
>
> > 如果记录成功，则更新内存，更新完成后会返回一个成功信号
> >
> > > 此时**fsimage**文件并没有改动
>
> **edits_inprogress**文件在达到一定的条件之后，会将其中记录操作的结果转化为元数据，更新到**fsimage**中，同时产生一个新的**edits_inprogress**，将原来的**edits_inprogress**重命名为**edits_XX-XX**

###### edits文件滚动的条件

> 空间
>
> > 当**edits_inprogress**文件达到一定的大小（**默认64M**），会自动进行一次滚动
>
> 时间
>
> > 当**edits_inprogress**文件距离上一次滚动的达到指定时间（**默认3600s**）,也会进行一次滚动，将操作结果记录到**fsimage**中
>
> 重启
>
> > 当NameNode重启的时候，会自动将**edits_inprogress**文件中的操作转化为结果记录到fsimage中
>
> 重启
>
> > 当**NameNode重启**的时候，会自动将**edits_inprogress**文件中的操作转化为结果记录到**fsimage**中
>
> 强制
>
> > 使用指令
> >
> > > `hadoop dfsadmin -rollEdits`

##### NameNode管理DataNode

> NameNode通过心跳机制来管理DataNode
>
> > DataNode会定时的向NameNode发送给**心跳信息**
> >
> > > NameNode实现心跳，DataNode调用
> >
> > 如果超过了指定的时间，NameNode没有收到DataNode的心跳，**NameNode**会认为这个**DataNode**已经**失效（丢失）**
> >
> > > 这时，**NameNode**会将这个**DataNode**身上存储的Block在其他节点上重新备份，保证副本数量
> >
> > **心跳**的时间是==3s==，如果超过10min没有收到信息，则认为节点丢失
>
> > **心跳信息**
> >
> > > 当前DataNode的**节点状态**
>
> DataNode通过RPC的方式给NameNode发送心跳











### 第二天

> HDFS

#### NameNode重启的时候

> 将**edits**中的操作转化为结果滚动到**fsimage**中
>
> 启动后读取**fsimage**，将元数据加载到**内存**中，等待**DataNode**心跳
>
> > 没有收到**DataNode**心跳，**NameNode**会认为这部分数据丢失，重新备份这些数据
>
> > 收到**DataNode**心跳，**NameNode**会==校验==**DataNode**上的数据是否准确合法
> >
> > > 如果==校验失败==，则试图恢复这个数据并且校验
> > >
> > > 如果==校验成功==，**NameNode**才会开始对外提供服务 ---->安全模式
> > >
> > > ==安全模式==
> > >
> > > > 安全模式中，**NameNode**会自动进行校验，如果一切校验成功，**NameNode**会自动退出安全模式
>
> 如果在合理时间内，没有退出==安全模式==，则这个时候就意味着数据产生丢失或不可恢复
>
> > 这个时候应该强制退出==安全模式==
> >
> > > `hadoop dfsadmin -safemode `
>
> 因为==安全模式==的存在，所以要在Hadoop的伪分布式中，副本数量必须置为1
>
> 在Hadoop2.X的完全分布式中，可以最多设置两个**NameNode**
>
> 在HDFS中，这个集群的负载量的多少基本上由**NameNode**的性能决定

#### 副本放置策略

> 第一个副本
>
> > 集群**内部上传**，谁上传，第一个副本就放在谁身上
> >
> > 集群**外部上传**，NameNode会从DataNode中选取一个相对空闲的节点来存储数据
>
> 第二个副本
>
> > Hadoop1.7之后，第二个副本放在与第一个副本**相同**的机架上
>
> 第三个副本
>
> > Hadoop1.7之后，第三个副本放在和第二个副本**不同**的机架上
>
> 更多副本
>
> > **NameNode**会选取相对空闲的节点来放置

#### 机架感知策略

> 所谓机架指的并不是物理机架，而是**逻辑机架**
>
> DataNode会每个**3s**向NameNode发送心跳
>
> DataNode会存在**四种状态**
>
> > ##### 预服役
> >
> > > 准备向集群中添加的节点
> >
> > ##### 服役
> >
> > > 集群中正在工作的节点
> >
> > ##### 预退役
> >
> > > 准备从急群众抽离的节点
> >
> > ##### 退役
> >
> > > 集群中抽离节点

#### SecondaryNameNode

> SecondaryNameNode**辅助**NameNode进行**edits**和**fsimmage**文件的滚动
>
> 在SecondaryNameNode**存在**的情况下，edits的滚动和写入fsimage的操作是发生在SecondaryNameNode上的

> SecondaryNameNode能起到一定的备份作用，但是不作为NameNode的**热备份**

> 在现在的开发环境中，**Hadoop**集群往往采用==双NameNode==机制来保证**NameNode**节点的==高可用==
>
> 一旦**NameNode**宕机，则HDFS整个集群会不能服务，所以要考虑到**NameNode**节点的高可用

#### 回收站机制

> 在HDFS中，回收站默认**不开启**
>
> 意味着如果删除文件**立马生效**
>
> 在core-site.xml中修改fs.trash.intercal属性来指定删除时间(min)

#### dfs目录

> dfs目录是在NameNode被**格式化**的时候出现额
>
> 存在三个**子目录**
>
> > data，name，namesecondary
>
> ##### in_use.lock
>
> > 表示当前节点**已经启动**对应的进程
> >
> > 防止多次启动

> **HDFS**启动，对每一次**写操作**分配一个全局事务id	-**txid**
>
> **HDFS**第一次启动的时候，1min之后会进行一次滚动，之后就会按照**fs.checkpoint.period**来进行滚动
>
> **edits**文件开始和结束都会分配一个**事务id**
>
> HDFS上的文件一旦上传完毕，不允许修改
>
> edits文件本身是一个**字节文件**，需要利用命令转化为指定格式后才能查看
>
> **fsimage_XXX**文件都会对应一个**fsimage_XXX.md5**
>
> > **MD5**文件使用md5算法对**fsimage**文件进行校验的

#### VERSION

> ##### ClusterID	- 标识DataNode是否归NameNode管理
>
> > 在HDFS中，当**NameNode被格式化**的时候，会产生一个ClusterID
>
> 在集群启动的时候
>
> > NameNode将**clusterID**分发给每一个DataNode，**DataNode**只会在开始的时候接收**ClusterID**
>
> NameNode在收到DataNode的信息之后
>
> > 先校验集群**ClusterID**是否一致。若一致，则接受信息
> >
> > 若不一致，则认为当前**DataNode**不归这个**NameNode**管理
>
> 如果多次格式化NameNode
>
> > 会导致一个集群中**NameNode**和**DataNode**的ClusterID不一致。此时需要统一**ClusterID**才能保证集群正常运行

> ##### blockpoolID	块池编号
>
> > ##### 联邦HDFS（Federation HDFS）
> >
> > > 多个节点起到一个NameNode的作用，这和设置方式称之为**Federation HDFS**
> >
> > 优势
> >
> > > 提高并发量
> >
> > 劣势
> >
> > > 不能动态增加路径
> > >
> > > 加大了网络资源的消耗
>
> > ##### 处在一个Federation HDFS的NameNode们，他们的blockpoolID必须一致

#### 并发，吞吐

> 并发
>
> > 线程数
>
> 吞吐
>
> > 读写速度
>
> 高并发不一定是高吞吐，高吞吐一定是高并发

#### 流程（读，写，删）

##### 读流程

> 客户端发器**RPC请求**到NameNode
>
> NameNode**查询元数据**，判断请求对应的路径是否存在
>
> 文件存在，NameNode会将**第一个Block**对应的存储地址放入**队列**中返回给客户端
>
> 客户端收到队列之后，取出Block对应的地址，从这些地址总选择一个**较近（中见经过转发次数较少的认为比较近）**的地址对用的DataNode来读取数据
>
> 客户端在读取完当前的Block之后，会进行一次checksum的验证，保证数据的完整性；
>
> > 如果校验失败，客户端会给NameNode报告错误信息，同时选取地址重新读取；
> >
> > 如果校验成功，则客户端会给NameNode发信息要下一个Block的地址
>
> 当客户端读取完所有的Block之后，客户端会通知NameNode关闭文件（实际上就是关流操作）
>
> ##### HDFS的读写流程之所以是让客户端连接DataNode，目的是为了提高吞吐量

##### 写流程

> 略

##### 删流程

> 略











### 第三天

#### MapReduce练习

##### 基础练习

> 统计每一个非空字符出现的次数
>
> ip去重
>
> 取最大值
>
> 求总分

##### 序列化练习 ---Writable

> 统计每一个人的总流量
>
> 统计每一个人的总分

##### 分区练习 --- Partitioner

> 按城市统计每一个人花费的总流量
>
> 按月统计每一个人的总分









### 第四天

#### MapReduce练习

> 打印每一个单词出现的文件

#### 序列化机制

> MapReduce中，要求被传输的数据必须能够被序列化
>
> 在MapReduce中，默认序列化机制是**AVRO**
>
> MapReduce对**AVRO**，进行了封装，提供了更简单的**Writable**接口

#### Partitioner	- 分区

> 分区的作用是对数据进行分类
>
> 在MapReduce中，**默认**只有一个**0分区**
>
> 每一个分区要对应一个**ReduceTask**，默认只有一个**ReduceTask**
>
> 每一个**ReduceTask**都只会产生一个结果文件
>
> 设置分区类，需要继承**Partitioner**类，覆盖其中的**getPartitioner**
>
> 分区是从**0**开始的，依次向上递增
>
> 在MapReduce中，分区类如果不指定，默认使用的是**HashPartitioner**

#### 排序

> MapReduce在计算过程中，会自动对键进行排序（**自然排序**）
>
> 如果需要自定义排序规则，那么对应的类需要实现**Comparable**。
>
> 考虑到需要进行初始化，所以通常实现的类是**WritableComparable**
>
> 在默认情况下，如果排序规则结果为**0**,则会认为这两个对象是**同一个键**

> 案例
>
> > 按照月份进行升序排序
> >
> > 如果是同一个月，那么按照业绩来进行降序排序
> >
> > > 文件：profit2.txt



#### 数据本地化策略

> 客户端将**MR**程序交给**JobTracker**
>
> **JobTracker**，找**NameNode**并索要处理文件的信息
>
> **NameNode**将信息返回**JobTracker**
>
> **JobTracker**将**MapTask**分配给**TaskTracker**(实际生产环境中将**DataNode**与**TaskTracker**部署在同一节点上)
>
> > 谁有这个数据就将任务分配给谁
>
> 扩展
>
> > 如果一个文件是一个空文件，则将文件整体作为一个切片
> >
> > 文件存可切以及不可切的问题
> >
> > > 例如**绝大多数的压缩文件是不能切片的**
> >
> > 如果一个文件不能切片，则这个文件整体作为一个切片进行处理
> >
> > **spiltSize**调小，改动**maxSize**；**spiltSpilt**调大，改动**minSize**
> >
> > 在切片的时候有一个阈值是1.1

#### Job执行流程

> 客户端先申请Job任务，同时提交jar包：`Hadoop jar xxx.jar`
>
> > 检查输入和输出路径
> >
> > 计算切片数量
> >
> > 如果有必要，设置缓存存根
> >
> > 将jar包和配置上传到HDFS上
> >
> > 提交任务，并且可以选择是否监控它的状态
>
> **JobTracker**收到任务之后，等待**MapTask**的心跳
>
> **JobTracker**收到**TaskTracker**心跳的时候，**JobTracker**就会给这个**TraskTracker**来分配任务
>
> **TaskTracker**领取到了任务，会连接对应的节点下载**jar包**
>
> **TaskTracker**在下载完jar包之后，会在开启JVM子进程,获取执行**MapTask**或者**ReduceTask**。
>
> > 每执行一次**MapTask**或者**ReduceTask**，默认会开启一次JVM子进程

#### Shuffle

> **Map端**的Shuffle
>
> > 每一个切片对应一个**MapTask**，在默认情况下,**MapReduce**在获取到切片之后进行读取，每读取一次，调用一次**map**方法
>
> > **map**在处理完一行数据之后，将处理结果写入缓冲区中，
> >
> > 果在**缓冲区中**进行分区，以及排序 -----缓冲区中的排序使用的是**快速排序**
>
> > 每一个**MapTask**都自带一个缓冲区，缓冲区是维系在内存中，默认还**100M**
>
> > 缓冲区的使用达到一定条件（**阈值为0.8**）的时候，会将缓冲区中的数据冲刷到**磁盘**中，这个过程称之为**溢写**，产生的文件称之为**溢写文件**
> >
> > 单个**溢写文件**中的数据也是分区且有序
>
> > **溢写**之后，**map**方法的后续结果继续写到缓冲区中，如果达到条件之后，会再次溢写
> >
> > **溢写**文件之间是**无序的**	-**局部有序**
>
> > 如果产生了多个**溢写文件**，那么**MapTask**在执行结束之后，会将这些溢写文件合并成一个**结果文件(final out)**，这个合并称之为**merge**
> >
> > > 如果没有产生溢写，则缓冲区中的数据直接冲刷到final out文件中
> >
> > 在**merge**过程中，数据会再次进行分区并且排序，因此final out文件是分区并且整体有序	-**将局部有序，变为整体有序**	- 使用的是**归并排序**
>
> 注意问题
>
> > **Spill**过程不一定会产生
> >
> > 一些文件的大小**不一定**等于缓冲区容量乘以阈值
> >
> > 缓冲区本质上是一个字节数据，而且是一个**环形的字节数组**
> >
> > 将缓冲区设置为环形的目的是为**减少寻址的时间**
> >
> > 阈值的目的是为了**避免阻塞**
>
> **Reduce端**的Shuffle
>
> > **ReduceTask**会启动**fetch**线程去**MapTask**端抓取数据，在抓取的时候只抓取当前**ReduceTask**对应的分区数据
>
> > **ReduceTask**将抓取来的数据放入文件中暂存，每一个**MapTask**专区的数据对应一个文件
>
> > 在抓取完成之后，**ReduceTask**会将这些小文件进行**merger**,合并成一个大文件
> >
> > 在**merge**的过程中，数据会再次进行**排序**，这次排序是将局部有序变为整体有序，依然使用的是**归并排序**
>
> > **merge**完成之后，会将相同的键对应的值放到一组，形成一个迭代器，这个过程称之为**分组**
>
> > **group**之后，每一个键都会调用一次**reduce**方法
>
> 注意问题
>
> > 默认**ReduceTask**会启动5个fetch
> >
> > **ReduceTask**启动阈值是0.05，当有**5%**的**MapTask**执行结束，就会启动**ReduceTask**
> >
> > **merge**因子默认是10，表示10个小文件合并成一个大文件

#### Shuffle的优化

> ###### 调大缓冲区，一般设置在**250~400M**之间
>
> 增大缓冲区的阈值	-意味着增大阻塞的几率
>
> ###### 适当的增加Combine的过程
>
> ###### 可以考虑将final out文件进行压缩，这种方式在网络条件不好情况下的一种取舍
>
> ###### 怎加fetch线程的数量
>
> 调小ReduceTask的启动阈值（不建议）
>
> 增大merge因子（不建议）









### 第五天

#### Combine	-合并

> **Combine**的目的是为了在保证结果不变的前提下，减轻**ReduceTask**的计算压力，提高计算效率

> 实际过程中，往往会将**Reduce**也作为**Combine**来使用，只需要在**Driver**添加`job.setCombinerClass()`就可以
>
> **Combine**的使用，要求不能改变执行的结果，就导致并不是所有的场景都适合于使用**Combine**
>
> **Combine**是减少计算的数据量，但是不改变结果

#### InputFormat	- 输入格式

> 代码见笔记

> 笔记有图

> **InputFormat**是**MapReduce**中顶级的输入格式，提供了2个抽象方法
>
> > **getSplits**（获取切片）和**createRecordReader**（针对切片产生输入流，用于读取切片）
> >
> > 针对文件的操作，更多的是实现**InputFormat**的子列**FileInputFormat**，这个子类中重写了**getSplits**方法，秩序自己覆盖**createRecordReader**
>
> 默认使用的是**TextInoutFormat**
>
> > 从第二个**MapTask**开始，每一个**MapTask**从当i请安的第二行开始到下一个切片的第一行，这样做的目的是为了保证数据的完整性
>
> 多元输入的前提下，允许数据对应的**输入格式**及**Mapper**不一样，但是**输出格式**要一致

#### 数据倾斜

> 代码详见笔记

> 数据本身具有倾斜的特性，所以在处理的过程中，会产生数据倾斜

> **Map端**数据倾斜产生的条件
>
> > 输入了多个文件，这些文件不可切，并且文件大小不均匀
> >
> > **Map端**产生数据倾斜，无法解决
>
> **Reduce端**产生数据倾斜
>
> > **Reduce端**，实际开发中，绝大多数的数据倾斜都发生在**Reduce**端
>
> > **Reduce端**数据倾斜的处理办法
> >
> > > 分阶段聚合
> >
> > > 连接join



#### 小文件的危害

> 存储	-HDFS
>
> > 小文件会产生**大量的元数据**，会加剧内存的消耗，甚至会导致内存被占满从而降低NameNode的处理效率
>
> 计算	-MapReduce
>
> > 每一个小文件会对应一个切片，一个切片对应一个**MapTask**，所以意味着如果有大量的小文件，则会产生大量的**MapTask**，每一个**MapTask**处理的数据量不大，但是**MapTask**多了可能会导致资源消耗加剧，甚至于服务器崩溃

> 小文件的处理手段常用的方式有两种
>
> > 合并和打包
>
> > Hadoop提供了一种原生的处理方式
> >
> > > HAR（Hadoop Archive）
> > >
> > > > 官方说法是将多个小文件发成一个har包，实际上是将多个小文件合并成一个大文件



#### 推测执行机制

> 推测执行机制实际上是Hadoop中针对“慢任务”的一种优化手段。如果出现了慢任务，则Hadoop将这个任务复制一份放到其他节点上执行，两个节点一起执行，谁先执行完成，则执行结果为最后结果，另一个没有完成的任务就会被kill掉
>
> 因为实际生产场景中，数据倾斜导致的慢任务更多，而此时推测执行机制并不能提高效率反而会导致集群资源的浪费，所以绝大部分情况下会关闭推测执行机制













### 第六天

#### Hadoop完全分布式搭建

> 详见老师笔记和课前资料

#### YARN

> 主要负责**协调/管理**和**任务调度**

#### YARN产生的原因

> 内因
>
> > **JobTracker**同时监控节点，任务数量是比较多的，导致整个集群的协调压力全部落到了**JobTracker**上
> >
> > 当**集群节点超过4000**个的时候，JobTracker会出现性能下降，甚至于崩溃的问题
>
> 外果
>
> > Hadoop发展过程中，产生了越来越多的**计算框架**，而很多计算框架都是基于HDFS来实现的
> >
> > **计算框架**之间的**计算模式**不一样，计算框架的设计必然会涉及到任务的划分和监控，以及资源的使用
> >
> > 为了解决不同框架之间任务冲突以及资源占用问题，需要设计一套统一的框架来**解决任务分配和资源分配问题**

#### 主要包含的三类进程

> ResourceManager
>
> > 用于进行资源管理
>
> NodeManager
>
> > 完全取代了TaskTracker，用于执行任务
>
> ApplicationMaster
>
> > 只有在任务需要执行的时候才会出现，负责进行任务的调度和监控

> **ResourceManager**管理**Application**，**ApplicationMaster**管理具体任务



#### Job执行流程

> 图见笔记

> **ResourceManager**接受任务，等待**NodeManager**心跳
>
> **ResourceManager**收到**NodeManager**心跳之后，进行响应，要求这个**NdoeManager**开启一个进程**ApplicationMaster**
>
> **NodeManager**在开启完**ApplicationMaster**之后会再次心跳
>
> **ResourceManager**在收到心跳之后会将**job**任务分配给这个**ApplicationMaster**
>
> **ResourceManager**在收到请求之后会进行资源的划分，将这个请求所需要的资源封装成一个**Container**对象（包含给这个任务所分配的内存大小以及CPU的核数，**默认一个任务分配1G内存+1CPU核**）
>
> **ApplicationMaster**收到**Container**之后，会将资源进行二次划分，划分给每一个**MapTask**和**ReduceTask**
>
> **ApplicationMaster**将**MapTask**和**ReduceTask**分配给**NodeManager**，并且监控这些任务的执行
>
> > 如果执行成功
> >
> > > 给**ApplicationMaster**一个成功的信号，并且释放资源
> >
> > 如果执行失败
> >
> > > 给**ApplicationMaster**一个失败的信号，并且释放资源
> > >
> > > 然后重新启动这个**MapTask**或者**ReduceTask**