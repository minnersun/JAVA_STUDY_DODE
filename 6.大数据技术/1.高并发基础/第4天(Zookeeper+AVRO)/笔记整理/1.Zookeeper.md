## Zookeeper

-----

### Zookeeper选举机制

#### 概述

> Zookeeper将选举拆分成了两个阶段
>
> >  数据回复阶段
> >
> > 选举阶段
>
> 数据恢复阶段
>
> > 服务器节点寻找当前节点中最大事务id
>
> 选举阶段
>
> > 每个节点会推荐自己当leader，并将节点信息发送其他节点
> >
> > 其他节点收到信息之后进行两两比较，进行多轮选举
> >
> > 最后胜出的节点会成为leader

#### 选举细节

##### 选举信息

> 当前节点的最大事务id
>
> 如果最大事务id一致，则比较myid
>
> 逻辑时钟值
>
> > 控制参加选举的节点都处在同一轮选举上

##### 比较原则

> 当前节点的最大事务id
>
> > 谁大选谁
>
> 如果最大事务id一致，则比较myid
>
> > 谁大选谁

##### 选举原则



> 当一个节点，胜过一半及以上的节点的时候，这个节点就会成为leader
>
> 后续添加的节点的事务id，myid无论是多少，都只能成为follower

##### 主节点宕机之后

> leader宕机之后，集群中会自动选举出一个新的leader，从而保证集群中不存在单点故障

##### 脑裂的产生及避免

> 脑裂的产生条件
>
> 集群分裂分裂的子集群中又进行了选举

> 在zookeeper集群中，如果存活的节点个数不足一半的时候，集群不对外提供服务，也不进行选举	- **过半性**
>
> 所以集群中的节点个数一般为奇数个	- 容易满足过半性

> 集群中会给选举出来的leader，分配一个全局递增的编号	-**epochid**
>
> leader会将**epochid**分发给每一个follower

> 如果产生了两个lead，可能是含有leader的集群被分裂了出去，另一半进行了选举
>
> 这样必然有一个leader的**epochid**是小于另一个leader的**epochid**
>
> 集群中会自动重启**epochid**小的leader

#### 节点的四种状态

> looking/voting	-选举状态
>
> following	-追随者/跟随着
>
> leader	-领导者
>
> observer	-观察者

##### 观察者的特点

> 既不参与投票也不参与选举
>
> 监听投票和选举的结果，根据结果来执行任务

> 使用场景
>
> > 集群节点数量众多，可以将一部分节点设置为观察者，
> >
> > 网络条件不稳定的情况下，可以将部分节点设置为观察者

> observer的存活与否并不影响集群的对外服务

##### Observer集群的配置

> 详见 课前资料 -观察者 - 配置

###### zoo.cfg

```
#2888为原子广播的端口 3888为选举端口
server.1=10.42.7.33:2888:3888
server.2=10.42.172.145:2888:3888
server.3=10.42.69.74:2888:3888
```



##### 查看集群中的事务的log文件

> 详见 课前资料 事务日志 -- 查看事务log







### ZAB协议

#### 概述

> ZAB（Zookeeper Atomic Broadcast）协议是专门针对Zookeeper的
>
> 用于原子广播和崩溃恢复的协议

> ZAB协议是基于2PC算法进行设计的，利用PAXOS算法进行了改进



#### 原子广播

> 老师笔记有图

###### 2PC

> 用于解决Zookeeper节点之间的**数据一致性**的问题
>
> 2PC - Two Phase Commit - 二阶端提交
>
> > 2PC将提交过程拆分成了两个阶段
> >
> > > 准备阶段
> > >
> > > > 协调者收到请求后，会将这个写请求分发给每一个参与者，等待参与者的反馈信息
> > >
> > > 提交阶段
> > >
> > > > 如果协调者收到所有的参与者返回的**yes**信号，则此时协调者就会给参与者发布命令，要求提交这个请求
> > >
> > > 终止阶段
> > >
> > > > 如果协调者没有收到所有参与者的**yes**。则此时协调者就会给所有的参与者发布命令，要求终止该操作
>
> 2PC的核心思想
>
> > 一票否决
>
> > 2PC的流程比简单，但是容易受到外界条件影响，效率相对较低

###### 原子广播

> 基于2PC，Zookeeper进行了改进，实现了原子广播

> 步骤
>
> leader
>
> > leader 收到写请求，将请求记录到本地的日志文件中
> >
> > 日志文件的存储位置由配置文件中的dataDir决定
>
> > 记录成功，leader节点认为这个请求可以执行
> >
> > 然后leader会将请求放入队列，发送给每一个follower
> >
> > 征求每一个follower的意见
>
> follower
>
> > follow在接收到队列之后
> >
> > 将队列中的请求取出，放入到本地日志文件中
>
> > 如果记录成功
> >
> > > follower会认为这个请求可执行，返回yes**信号**
> >
> > 如果记录失败
> >
> > > follower认为请求不能执行，返回**no**信号
>
> leader
>
> > 收到一半及以上的节点返回**yes**
> >
> > > leader会命令每一个节点执行这个请求
> >
> > 没有收到一半以上的节点返回**yes**
> >
> > > leader会认为这个请求不能执行
> > >
> > > 要求所有节点删除对应的日志
>
> 如果某个节点记录日志失败，但是又需要执行该请求
>
> 则这个节点会再给leader发送信息，要求操作
>
> 接收到操作之后，重新记录日志，重新执行





### 崩溃恢复

> 当集群中的leader产生丢失的时候，现在满足条件的前提下，集群会自动选举出一个新的leader
>
> 崩溃恢复解决了单点故障问题

> 在崩溃恢复过程中，新选举出来的leader会分配一个递增的**epochid**
>
> 在集群中，**事务id** 实际上是由64位二进制数字（16位十六进制）组成
>
> > 其中高32位表示**epochid**
> >
> > 低32位表示的是**事务id**

> 如果某个节点**重新启动**
>
> > 那么启动后，这个节点会寻找当前节点的**最大事务id**
> >
> > 会在发消息给leader比较**事务id**是否一致
> >
> > > 如果**事务id**一致
> > >
> > > > 说明数据一致
> > >
> > > 如果**事务id**不一致
> > >
> > > > leader会将差出的操作放入队列中发送给该节点
> > > >
> > > > 在该节点恢复数据阶段，该节点不对外服务